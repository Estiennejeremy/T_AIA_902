{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.99\n",
    "min_epsilon = 0.01\n",
    "max_epsilon = 1.0\n",
    "decay_rate = 0.01\n",
    "\n",
    "total_episodes = 10000\n",
    "max_steps = 100\n",
    "\n",
    "lr_rate = 0.7\t# 0.81\n",
    "gamma = 0.99\t# 0.96\n",
    "\n",
    "qtable = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "\taction=0\n",
    "\tif np.random.uniform(0, 1) < epsilon:\n",
    "\t\taction = env.action_space.sample()\n",
    "\telse:\n",
    "\t\taction = np.argmax(qtable[state, :])\n",
    "\treturn action\n",
    "\n",
    "def learn(state, state2, reward, action, action2):\n",
    "\tpredict = qtable[state, action]\n",
    "\ttarget = reward + gamma * qtable[state2, action2]\n",
    "\tqtable[state, action] = qtable[state, action] + lr_rate * (target - predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.59258118e-01 1.36068824e-01 1.41521259e-01 2.42443945e-01]\n",
      " [3.07050325e-02 5.37284593e-02 1.02747002e-02 2.49515175e-01]\n",
      " [5.20784551e-02 8.40248003e-02 4.57110523e-02 1.76227205e-01]\n",
      " [1.32587931e-02 4.82909404e-02 1.74200156e-03 1.51123923e-01]\n",
      " [6.49463775e-01 1.12256548e-01 1.45987249e-01 9.28390625e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.58797598e-05 4.36293518e-03 4.74238106e-03 6.72799915e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [5.11208328e-02 6.58787439e-03 5.74352586e-02 8.37148397e-01]\n",
      " [1.32617516e-02 7.82610801e-01 1.59359437e-04 5.32592244e-02]\n",
      " [9.15869064e-01 1.32356173e-02 2.36028880e-03 8.23698181e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [4.99141883e-02 6.24475765e-02 9.79828504e-01 1.83958462e-01]\n",
      " [3.22898839e-01 9.99832209e-01 4.26074763e-01 4.77120453e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "rewards=0\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\tt = 0\n",
    "\tstate = env.reset()\n",
    "\taction = choose_action(state)\n",
    "    \n",
    "\twhile t < max_steps:\n",
    "\n",
    "\t\tstate2, reward, done, info = env.step(action)\n",
    "\n",
    "\t\taction2 = choose_action(state2)\n",
    "\n",
    "\t\tlearn(state, state2, reward, action, action2)\n",
    "\n",
    "\t\tstate = state2\n",
    "\t\taction = action2\n",
    "\n",
    "\t\tt += 1\n",
    "\t\trewards+=1\n",
    "\n",
    "\t\tif done:\n",
    "\t\t\tepsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "\t\t\t#print(epsilon)\n",
    "\t\t\tbreak\n",
    "\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "max_iteration = 100\n",
    "victory = 0\n",
    "steps = []\n",
    "\n",
    "for episode in range(max_iteration):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            # env.render()\n",
    "            if reward == 1:\n",
    "                victory += 1\n",
    "                steps.append(step)\n",
    "            break\n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage de réussite en mode QLearning :  77.0 %\n",
      "Nombre moyen de pas pour arriver au but :  38.87012987012987\n",
      "Nombre median de pas pour arriver au but :  39.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pourcentage de réussite en mode QLearning : \", victory / max_iteration * 100, \"%\")\n",
    "print(\"Nombre moyen de pas pour arriver au but : \",np.mean(steps))\n",
    "print(\"Nombre median de pas pour arriver au but : \",np.median(steps))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
